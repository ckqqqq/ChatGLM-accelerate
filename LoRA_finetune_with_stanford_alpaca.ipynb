{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model From huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a5caf2aa974f73959245c3ac4a9b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)/modeling_chatglm.py:   0%|          | 0.00/50.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9986766d2648e8a11d482690fcf850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7140268828cd41cc97cc3268ea5b13a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "checkpoint = \"THUDM/chatglm-6b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(checkpoint, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert LoRA to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "import loralib as lora\n",
    "from peft import LoraConfig\n",
    "\n",
    "\n",
    "config = LoraConfig(\n",
    "              peft_type=\"LORA\", \n",
    "              task_type=\"SEQ_2_SEQ_LM\", \n",
    "              r=32, \n",
    "              lora_alpha=32, \n",
    "              target_modules=[\"q\", \"k\", \"v\"],\n",
    "              lora_dropout=0.1, \n",
    "              )\n",
    "\n",
    "\n",
    "class QKV_layer(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(QKV_layer, self).__init__()\n",
    "        self.linear_q = torch.nn.Linear(in_features, out_features//3)\n",
    "        self.linear_k = torch.nn.Linear(in_features, out_features//3)\n",
    "        self.linear_v = torch.nn.Linear(in_features, out_features//3)\n",
    "\n",
    "    def update(self, target_layer):\n",
    "        self.linear_q.weight.data = target_layer.weight[:target_layer.out_features//3, :].data\n",
    "        self.linear_q.bias.data = target_layer.bias[:target_layer.out_features//3].data\n",
    "\n",
    "        self.linear_k.weight.data = target_layer.weight[target_layer.out_features//3:target_layer.out_features//3*2, :].data\n",
    "        self.linear_k.bias.data = target_layer.bias[target_layer.out_features//3:target_layer.out_features//3*2].data\n",
    "\n",
    "        self.linear_v.weight.data = target_layer.weight[target_layer.out_features//3*2:, :].data\n",
    "        self.linear_v.bias.data = target_layer.bias[target_layer.out_features//3*2:].data\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q = self.linear_q(x)\n",
    "        k = self.linear_k(x)\n",
    "        v = self.linear_v(x)\n",
    "        return torch.concat([q,k,v], dim = -1)\n",
    "\n",
    "\n",
    "for key, module in model.named_modules():\n",
    "    if key.endswith('attention'):\n",
    "        try:\n",
    "            # Here we split the query_key_value layer into three linear layer for LoRA. But you can also use merged linear.\n",
    "            qkv_layer = QKV_layer(module.query_key_value.in_features, module.query_key_value.out_features) \n",
    "            qkv_layer.update(module.query_key_value)\n",
    "            module.query_key_value = qkv_layer\n",
    "        except:\n",
    "            pass\n",
    "        module.query_key_value = peft.tuners.lora.LoraModel(config, module.query_key_value)\n",
    "\n",
    "\n",
    "lora.mark_only_lora_as_trainable(model)\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "trainable_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "\n",
    "model_parameters = filter(lambda p: not p.requires_grad, model.parameters())\n",
    "non_trainable_params = sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable_params:22020096 (0.35%), non_trainable_params:6255206400\n"
     ]
    }
   ],
   "source": [
    "print('trainable_params:{} ({:.2f}%), non_trainable_params:{}'.format(trainable_params, trainable_params/non_trainable_params*100,non_trainable_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "device = 'cuda'\n",
    "EOS_ID = 150005\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "with open('data/alpaca_data.json', 'r') as f:\n",
    "    content = json.load(f)\n",
    "\n",
    "\n",
    "pairs = []\n",
    "\n",
    "for line in content:\n",
    "    if line['input'] == '':\n",
    "        prompt = PROMPT_DICT['prompt_no_input'].format_map(line)\n",
    "    else:\n",
    "        prompt = PROMPT_DICT['prompt_input'].format_map(line)\n",
    "    completion = line['output']\n",
    "    pairs.append({'prompt':prompt, 'completion':completion})\n",
    "\n",
    "\n",
    "class AlpacaDataset(Dataset):\n",
    "    def __init__(self, pairs, tokenizer) -> None:\n",
    "        super().__init__()\n",
    "        self.pairs = pairs\n",
    "        self.tokenizer = tokenizer\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        prompt = self.tokenizer.encode(self.pairs[index]['prompt'])\n",
    "        completion = self.tokenizer.encode(self.pairs[index]['completion'], add_special_tokens=False) + [EOS_ID]\n",
    "\n",
    "        seq = prompt + completion\n",
    "        context_length = seq.index(150004) + 1\n",
    "\n",
    "        attention_mask = torch.ones((len(seq), len(seq)), device=device)\n",
    "        attention_mask.tril_()\n",
    "        attention_mask[..., :context_length - 1] = 1\n",
    "        attention_mask.unsqueeze_(0)\n",
    "        attention_mask = (attention_mask < 0.5).bool()\n",
    "\n",
    "        position_ids = torch.stack([torch.arange(0,len(seq), device=device), torch.concat([torch.zeros(context_length-2, device=device), torch.arange(0,len(seq)-context_length+2, device=device)])]).long()\n",
    "        labels = torch.tensor([-100] * len(prompt) + completion, device=device).long()\n",
    "\n",
    "        return {'input_ids':seq, 'attention_mask':attention_mask, \"labels\":labels, 'position_ids':position_ids}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "    position_ids = []\n",
    "    # TODO: padding for batch training\n",
    "    for obj in batch:\n",
    "        input_ids.append(obj['input_ids'])\n",
    "        attention_mask.append(obj['attention_mask'])\n",
    "        labels.append(obj['labels'])\n",
    "        position_ids.append(obj['position_ids'])\n",
    "    return {'input_ids': torch.tensor(input_ids).long(), \n",
    "            'attention_mask': torch.stack(attention_mask), \n",
    "            'labels': torch.stack(labels),\n",
    "            'position_ids':torch.stack(position_ids)}\n",
    "\n",
    "            \n",
    "\n",
    "train_dataset = AlpacaDataset(pairs,tokenizer=tokenizer,)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, collate_fn = collate_fn, shuffle=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show how to train the model with gradient accumulation as well as mix precision, and then save the model (only LoRA's weight which is typically within 10MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "LR = 2e-5\n",
    "NUM_EPOCHS = 2\n",
    "accumulate_step = 32\n",
    "version = 'test'\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(len(train_dataloader) / accumulate_step),\n",
    "    num_training_steps=(int(len(train_dataloader) / accumulate_step) * NUM_EPOCHS),\n",
    ")\n",
    "\n",
    "\n",
    "model.to(device).train()\n",
    "\n",
    "with autocast(dtype=torch.bfloat16):\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        total_loss = 0\n",
    "        for step, batch in enumerate(t:=tqdm.tqdm(train_dataloader)):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss_d = outputs.loss.detach().float()\n",
    "            t.set_description(f\"loss: {loss_d}\")\n",
    "            total_loss += loss_d\n",
    "            loss = outputs.loss / accumulate_step\n",
    "            loss.backward()\n",
    "            if (step+1) % accumulate_step == 0:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        peft_model_id = f\"{checkpoint}_{version}_{epoch}\"\n",
    "        torch.save(lora.lora_state_dict(model), peft_model_id+'.pt')\n",
    "        print(epoch, total_loss/(step+1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "\n",
    "# convert it again\n",
    "for key, module in model.named_modules():\n",
    "    if key.endswith('attention'):\n",
    "        try:\n",
    "            qkv_layer = QKV_layer(module.query_key_value.in_features, module.query_key_value.out_features) \n",
    "            qkv_layer.update(module.query_key_value)\n",
    "            module.query_key_value = qkv_layer\n",
    "        except:\n",
    "            pass\n",
    "        module.query_key_value = peft.tuners.lora.LoraModel(config, module.query_key_value)\n",
    "\n",
    "\n",
    "# load the LoRA checkpoint\n",
    "model.load_state_dict(torch.load('.pt file you saved'), strict=False)\n",
    "\n",
    "model.half().cuda().eval()\n",
    "\n",
    "# Let's chat!\n",
    "response, history = model.chat(tokenizer, \"Hello\", history=[])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ckqpy38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d8bdb44f3e9a5714bc2974b8edd400dea31ee2ae63ed4830807e434c750c669"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
